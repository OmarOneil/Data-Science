{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarOneil/Data-Science/blob/main/Production_Ready_RAG_and_LangSmith.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Production Ready RAG and LangSmith\n",
        "\n",
        "Today we'll take a peek at ways we can improve typical Retrieval Augmented Generation pipelines - and showcase how we can test our pipelines to provide directional signal!"
      ],
      "metadata": {
        "id": "HoxOG15eIe0L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbQCV9GHQKB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48962176-b690-4961-ef73-341d29953ffa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -U -q langchain openai langsmith cohere tiktoken qdrant-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to make sure we can run async within our Jupyter Notebook - so we'll do that in the next cell!"
      ],
      "metadata": {
        "id": "8HrKKvBQKcjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "oo4r2s025UWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Document Loader\n",
        "\n",
        "Now we can load our data source - OpenAI [blogs](https://openai.com/blog) - for that, we'll use our SitemapLoader which will be able to parse out the OpenAI sitemap, and then filter out only the blog posts!"
      ],
      "metadata": {
        "id": "1k7-f05eLrC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders.sitemap import SitemapLoader\n",
        "\n",
        "loader = SitemapLoader(\n",
        "    web_path = \"https://openai.com/sitemap.xml\",\n",
        "    filter_urls=[\"https://openai.com/blog\"]\n",
        ")"
      ],
      "metadata": {
        "id": "OIRxUIP65B0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HUyhxYLB5QPb",
        "outputId": "7e7243fb-7750-4350-e904-e6cf1694c06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching pages: 100%|##########| 113/113 [00:07<00:00, 14.98it/s]\n",
            "/usr/local/lib/python3.10/dist-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Tpwqh7J7zIt",
        "outputId": "75d77dee-dab4-4d73-b807-22c28b0b071a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "113"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have ~113 blog posts loaded up - and now we need to cut them down to a reasonable chunk size.\n",
        "\n",
        "We'll use the rather naive RecursiveCharacterTextSplitter to achieve this goal today.\n",
        "\n",
        "As we know our blogs are in a typical writtern format - paragraphs, sentences, headed sections - we can split preferentially by `\\n\\n`, `\\n`, `' '`."
      ],
      "metadata": {
        "id": "ZKzVkHqnMdqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 1250,\n",
        "    chunk_overlap  = 100,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")"
      ],
      "metadata": {
        "id": "cnnfEHsh7aqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can split our blogs!"
      ],
      "metadata": {
        "id": "NfKPVtaRNA2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naive_split_docs = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "aDPiFS_J7pEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(naive_split_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lJ4JVgZ7xog",
        "outputId": "9c37301b-08a8-4f03-ff0a-359bce62c98d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "831"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've got a final number of 831 chunks!"
      ],
      "metadata": {
        "id": "JN-UnJLiND0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Embeddings\n",
        "\n",
        "We'll be leveraging [Cohere's Embeddings v3](https://txt.cohere.com/introducing-embed-v3/) embeddings model.\n",
        "\n",
        "It's, as of time of writing this notebook, the most performant closed-source embeddings model available!"
      ],
      "metadata": {
        "id": "KzX-YramNH6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to start by providing our Cohere API key!"
      ],
      "metadata": {
        "id": "b9AEDW7dONwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['COHERE_API_KEY'] = getpass.getpass('Enter your Cohere API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkLb1djU68pi",
        "outputId": "a8810a98-d6e2-4fa0-a5c2-fc4c9e8ed20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Cohere API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can load our embeddings model - we'll be leveraging the \"light\" model to reduce cost"
      ],
      "metadata": {
        "id": "OBfgEl0EOQxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import CohereEmbeddings\n",
        "\n",
        "embeddings = CohereEmbeddings(model=\"embed-english-light-v3.0\")"
      ],
      "metadata": {
        "id": "e9Fh_3ol6684"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vector Store & Retriever\n",
        "\n",
        "For our vector store today we'll be using Qdrant!\n",
        "\n",
        "To keep things consistent in the notebook, we'll be leveraging their cloud solution - which provides 1GB of free-tier access.\n",
        "\n",
        "Qdrant is an open-source, self-hostable, performant vector database.\n",
        "\n",
        "If you listen to their [marketing](https://qdrant.tech/benchmarks/?gad_source=1&gclid=CjwKCAiA1MCrBhAoEiwAC2d64Xro4dyNYPXWzmAkaqQMDEfzrjjLaMKHW0LhtMpJEvQTAETbws2RaBoC1aAQAvD_BwE) they are among the best of the best.\n",
        "\n",
        "In reality, Qdrant can scale to extremely high volumes without performance suffering - and retains the option to self-host, which can be critical for businesses with data or privacy concerns.\n",
        "\n",
        "Let's get started by loading our API key and our cluster URL."
      ],
      "metadata": {
        "id": "D9kCZkycOjrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_api_key = getpass.getpass(\"QDrant Cluster API Key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtD_Hj-5704e",
        "outputId": "cb218f92-5b2d-402b-876b-5526a47a5fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDrant Cluster API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qdrant_cluster_url = getpass.getpass(\"QDrant Cluster URL: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLlSre5l8EPk",
        "outputId": "763991e3-ce0a-4841-e90a-d17a6cf8ce12"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QDrant Cluster URL: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can instantiate our Qdrant cluster from LangChain!"
      ],
      "metadata": {
        "id": "FtSNETurPgu8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Qdrant\n",
        "\n",
        "qdrant = Qdrant.from_documents(\n",
        "    naive_split_docs,\n",
        "    embeddings,\n",
        "    url=qdrant_cluster_url,\n",
        "    prefer_grpc=True,\n",
        "    api_key=qdrant_api_key,\n",
        "    collection_name=\"openai_blogs\",\n",
        ")"
      ],
      "metadata": {
        "id": "6DWclVzv8PDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll set this as our base retriever - and set the number of retrieved documents (typically called `k`) to a high number for use with our reranker later on."
      ],
      "metadata": {
        "id": "zc3Xw0UxPlUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_retriever = qdrant.as_retriever(search_kwargs={\"k\" : 20})"
      ],
      "metadata": {
        "id": "FrOYMjy38ovl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble Retrieval\n",
        "\n",
        "In order to augment our retrieval stack - we're going to leverage something called \"ensemble retrieval\".\n",
        "\n",
        "The basic idea is as follows:\n",
        "\n",
        "1. Retrieve a large number of documents from a dense vector retrieval.\n",
        "2. Retrieve a large number of documents from a sparse vector search.\n",
        "3. Use [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) to combine the results into a single ranked set.\n",
        "\n",
        "We'll use our Qdrant vector-database to power our dense vector retrieval - and we'll use [BM25](https://en.wikipedia.org/wiki/Okapi_BM25) as our sparse solution.\n",
        "\n",
        "LangChain will take care of the rest."
      ],
      "metadata": {
        "id": "EnNOsZjvPzWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU rank_bm25"
      ],
      "metadata": {
        "id": "YHgOCRd0_v_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(naive_split_docs)\n",
        "bm25_retriever.k = 20"
      ],
      "metadata": {
        "id": "SDbhkuF1_pF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[bm25_retriever, base_retriever], weights=[0.5, 0.5]\n",
        ")"
      ],
      "metadata": {
        "id": "VNmdrOqG_4Mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(ensemble_retriever.invoke(\"How many parameters were in GPT, GPT-2, InstructGPT, and GPT-3 models?  What were other key differences?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRV3dxJLBXp3",
        "outputId": "b76fbada-5dcc-4449-c6e2-7b28c8d8083f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Re-ranking\n",
        "\n",
        "Now that we have a large number of retrieved documents - we can use Cohere's [Rerank](https://txt.cohere.com/rerank/) service to provide us with a reranked list of the top 5 most relevant sources.\n",
        "\n",
        "This idea of \"casting a wide net\" and then trimming down the results will help us improve our results fairly significantly."
      ],
      "metadata": {
        "id": "6CoFKLsLQyQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(top_n=5)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
        ")"
      ],
      "metadata": {
        "id": "Kx6--dS381eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(compression_retriever.invoke(\"How many parameters were in GPT, GPT-2, InstructGPT, and GPT-3 models?  What were other key differences?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMo2A-hgBP78",
        "outputId": "90c28364-9df8-4230-ff13-0430439733fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating our Chain\n",
        "\n",
        "Now that we have our retrieval pipeline, we can integrate it into a chain - and leverage that to ask questions about our data!\n",
        "\n",
        "First, we'll set up a chat template that is compatible with the RAG pattern:\n",
        "\n",
        "We'll provide a user question, then we'll provide relevant context that will be used to answer the question!"
      ],
      "metadata": {
        "id": "_EtuC7STRGBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "xqKRU0wU-AIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNFxvMbpQKB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd6f6ee-5d50-41e9-e046-5255ba674773"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll want to set up the \"brains\" of the operation - GPT-4 Turbo!\n",
        "\n",
        "Once again, we'll use LangChain to make this easy!"
      ],
      "metadata": {
        "id": "9pNTQrYNS_dK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4-1106-preview\", temperature=0)"
      ],
      "metadata": {
        "id": "7dqQtzcj9phL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our chain!\n",
        "\n",
        "You'll notice we're using the LCEL to do this - this is the prefered method of initializing chains for production with LangChain.\n",
        "\n",
        "More information is provided [here](https://python.langchain.com/docs/expression_language/)!"
      ],
      "metadata": {
        "id": "1wstxW5fTkz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "rerank_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "xwTQ4knK-JLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can invoke our chain - and see what kinds of outputs we get!"
      ],
      "metadata": {
        "id": "Dlg7K2yrU42o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_rag_chain.invoke({\"question\" : \"What are Sam Altman's thoughts on the recent leadership transition?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "FBiI5GMm_-mj",
        "outputId": "c3c7626b-dfe8-4c3e-ec3e-a1d488acb1ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, Sam Altman\\'s thoughts on the recent leadership transition are positive and forward-looking. He expresses excitement about the future and gratitude for the team\\'s hard work during an unclear and unprecedented situation. He mentions his belief in the resilience and spirit of the team, which he feels sets them apart. Altman is looking forward to continuing the work on building beneficial artificial general intelligence (AGI) with what he refers to as \"the best team in the world, best mission in the world.\" His message conveys a sense of optimism and commitment to the mission of OpenAI.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsyvZ0VWQKB1"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q arxiv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DXz1S02QKBy"
      },
      "source": [
        "# LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJKu0EIyQKB1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "848910c3-2b05-4137-ac58-3197c70e569f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"LangSmith Introduction - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVg4ZR06QKB1"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_rag_chain.invoke({\"question\" : \"what are the ethical and alignment considerations that I should keep in mind when training and fine-tuning my own LLM?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "xzOIa3hrCjGU",
        "outputId": "e2bc7b6f-164e-4cca-d47f-1eac098ff697"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"When training and fine-tuning your own Large Language Model (LLM), you should consider the following ethical and alignment considerations:\\n\\n1. Prohibit misuse: Establish usage guidelines and terms of use that prevent material harm to individuals, communities, and society. This includes prohibiting the use of LLMs for spam, fraud, astroturfing, or any high-risk use-cases that are not appropriate, such as classifying people based on protected characteristics.\\n\\n2. Enforce usage guidelines: Build systems and infrastructure to enforce the guidelines you set. This could involve rate limits, content filtering, application approval processes, monitoring for anomalous activity, and other mitigations.\\n\\n3. Mitigate unintentional harm: Take proactive steps to mitigate harmful model behavior. This includes comprehensive model evaluation to understand limitations, minimizing potential sources of bias in training data, and employing techniques to minimize unsafe behavior, such as learning from human feedback.\\n\\n4. Document weaknesses and vulnerabilities: Be transparent about the model's limitations, including biases or the potential to produce insecure code. No preventative action can completely eliminate the potential for unintended harm, so documentation is crucial.\\n\\n5. Collaborate with stakeholders: Build teams with diverse backgrounds and solicit broad input to ensure that the LLM operates effectively and fairly across diverse real-world scenarios. Diverse perspectives can help identify and address potential biases and operational failures for certain groups.\\n\\n6. Public disclosure: Share lessons learned regarding LLM safety and misuse to enable widespread adoption and help with cross-industry iteration on best practices.\\n\\n7. Respect labor: Maintain high standards for the working conditions of those involved in the LLM supply chain, including in-house reviewers of model outputs and external vendors. Ensure that workers can opt out of tasks when necessary.\\n\\n8. Continuous learning and updating: Recognize that the commercial uses of LLMs and accompanying safety considerations are new and evolving. Stay actively engaged in learning about and addressing LLM limitations and avenues for misuse, and update principles and practices in collaboration with the broader community over time.\\n\\nThese considerations are designed to guide safer development and deployment of LLMs, reduce unintentional harms, and prevent malicious use. It's important to approach these considerations as part of an ongoing commitment to ethical AI practices and to adapt as the technology and its applications evolve.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_rag_chain.invoke({\"question\" : \"what are most important recent advancements related to building production LLM applications?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "bTsvUxF-Cn0o",
        "outputId": "156fcf0b-f086-4bd9-fab4-b8f6f3d7c58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, the most important recent advancements related to building production Large Language Model (LLM) applications are not explicitly listed. However, the documents do discuss best practices for deploying LLMs, which can be seen as advancements in the responsible and safe use of these models in production environments. These best practices include:\\n\\n1. Prohibiting misuse by publishing usage guidelines and terms of use that prevent material harm through actions like spam, fraud, or astroturfing.\\n2. Building systems and infrastructure to enforce usage guidelines, such as rate limits, content filtering, and monitoring for anomalous activity.\\n3. Mitigating unintentional harm by conducting comprehensive model evaluations, minimizing bias in training data, and learning from human feedback.\\n4. Documenting known weaknesses and vulnerabilities of the models to inform users and developers.\\n5. Collaborating with diverse stakeholders to address potential biases and ensure the models work effectively for all groups.\\n6. Publicly disclosing lessons learned regarding LLM safety and misuse to foster cross-industry collaboration on best practices.\\n7. Treating all labor in the language model supply chain with respect, including setting high standards for working conditions and allowing opt-outs for certain tasks.\\n\\nThese practices represent a step towards more ethical and effective deployment of LLMs in production, focusing on safety, fairness, and collaboration. They are not technological advancements per se, but rather advancements in the approach to deploying and managing LLMs in real-world applications.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rerank_rag_chain.invoke({\"question\" : \"How many parameters were in GPT, GPT-2, InstructGPT, and GPT-3 models?  What were other key differences?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "vnKnt2zaC_fS",
        "outputId": "b967cf17-9234-48ae-aa19-d1639ca13361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the provided context, Sam Altman's thoughts on the recent leadership transition are positive and forward-looking. He expresses excitement about the future and gratitude for the team's hard work during an unclear and unprecedented situation. He mentions his belief in the resilience and spirit of the team, setting them apart, and looks forward to working closely with the new initial board and the OpenAI community to continue building beneficial artificial general intelligence (AGI). He signs off with a message of love, indicating a personal and emotional investment in the company and its mission.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVp88CIpQKB3"
      },
      "source": [
        "Let's build a number of input/output pairs that we can leverage later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6pX2MoaQKB3"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "\n",
        "inputs = [\n",
        "    \"What are Sam's thoughts of Illya?\",\n",
        "    \"What are some frontier risks?\",\n",
        "    \"What are Custom GPTs?\",\n",
        "    \"Can I use DALL-E 3 with ChatGPT Plus?\",\n",
        "    \"What is 'red teaming'?\",\n",
        "    \"How can AI be leverages to do better teaching?\"\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "async def arun(chain, input_example):\n",
        "    try:\n",
        "        return await chain.invoke({\"question\" : input_example})\n",
        "    except Exception as e:\n",
        "        return e\n",
        "\n",
        "for input_example in inputs:\n",
        "    results.append(arun(rerank_rag_chain, input_example))\n",
        "\n",
        "results = await asyncio.gather(*results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNYatDN2QKB3"
      },
      "source": [
        "Now that we've run through all of those chains - we can leverage LangSmith to create a dataset that we can use to benchmark other application solutions!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgGCYxekQKB3"
      },
      "outputs": [],
      "source": [
        "from langchain.callbacks.tracers.langchain import wait_for_all_tracers\n",
        "\n",
        "wait_for_all_tracers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM18lZq3QKB3"
      },
      "source": [
        "### Evaluating with LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lf4Lx9gQKB3"
      },
      "source": [
        "The first thing we'll need to do is collect our responses into a dataset that we can use to benchmark other solutions against!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNJJkgiuQKB3"
      },
      "outputs": [],
      "source": [
        "dataset_name = f\"openai-rag-{unique_id}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name, description=\"A dataset for benchmarking a RAG system using the OpenAI Blogs as Source Material\"\n",
        ")\n",
        "\n",
        "runs = client.list_runs(\n",
        "    project_name=os.environ[\"LANGCHAIN_PROJECT\"],\n",
        "    execution_order=1,  # Only return the top-level runs\n",
        "    error=False,  # Only runs that succeed\n",
        ")\n",
        "for run in runs:\n",
        "    client.create_example(inputs=run.inputs, outputs=run.outputs, dataset_id=dataset.id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFuRRG-WQKB3"
      },
      "source": [
        "Now that we have our dataset set up in LangSmith - let's create another system that we can benchmark against our original!\n",
        "\n",
        "Since it's possible to build an agent that has memory (which could influence results and might not provide accurate benchmarking) - we'll use an `agent_factory` to create our agent for each test-case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVV3T6owQKB4"
      },
      "outputs": [],
      "source": [
        "def chain_factory():\n",
        "    rerank_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    )\n",
        "    return rerank_rag_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3W8XIlnQKB4"
      },
      "source": [
        "Now we can use the `langchain.evaluation.EvaluatorType` and `langchain.smith.RunEvalConfig` methods to build a pipeline for our evaluation.\n",
        "\n",
        "More information about these metrics is found [here](https://docs.smith.langchain.com/evaluation/evaluator-implementations)\n",
        "Let's set it up with the following evluators:\n",
        "\n",
        "- `EvaluatorType.QA` - measures how \"correct\" your response is, based on a reference answer (we built these in the first part of the notebook)\n",
        "- `EvaluatorType.EMBEDDING_DISTANCE` - measure closeness between the two responses\n",
        "- `RunEvalConfig.LabeledCriteria` - measures the output against the given criteria\n",
        "- `RunEvalConfig.Criteria({\"YOUR CUSTOM CRITERAI\", \"DESCRIPTION OF YOUR CRITERIA IN NATURAL LANGUAGE\"})`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4JUAH5RQKB4"
      },
      "source": [
        "We'll also build our own custom evaluator as a demonstration of how to implement such an evaluator!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiGbPZAKQKB4"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lB1etMxQKB4"
      },
      "source": [
        "In our own custom evaluator we need to make sure of a couple things:\n",
        "\n",
        "1. We provide a system by which we can measure or provide a measure of closeness/some numeric metric.\n",
        "2. We provide logic for implementing our score and parsing the relevant outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "smIHwGbGQKB4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = LLMChain.from_string(llm=llm, template=template)\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"dopeness_score\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain(\n",
        "            dict(input=input, prediction=prediction), **kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result[\"text\"].split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"dopeness\": reasoning.strip()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RepATL_QKB4"
      },
      "source": [
        "Now we can set our `RunEvalFeedback` up!\n",
        "\n",
        "Notice how we can create custom evaluations that are string based only -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFTYVFCjQKB4"
      },
      "outputs": [],
      "source": [
        "from langchain.evaluation import EvaluatorType\n",
        "from langchain.smith import RunEvalConfig\n",
        "\n",
        "evaluation_config = RunEvalConfig(\n",
        "    evaluators = [\n",
        "        EvaluatorType.QA,\n",
        "        EvaluatorType.EMBEDDING_DISTANCE,\n",
        "        RunEvalConfig.LabeledCriteria(\"relevance\"),\n",
        "        RunEvalConfig.Criteria({\n",
        "            \"fully_answered\" : \"Does this response fully answer the question?\"\n",
        "        })\n",
        "    ],\n",
        "    custom_evaluators = [\n",
        "        DopenessEvaluator()\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jL4K4qBqQKB4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0b9dca-9058-4ff4-9514-531fa03d657f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'cooked-insurance-16' at:\n",
            "https://smith.langchain.com/o/69867b2b-1696-431d-a878-81df6e9d559b/datasets/da76ed52-268a-4504-9f84-a5aa4030c45e/compare?selectedSessions=ad1f7674-9eac-4ac3-a4d2-ccde1717c753\n",
            "\n",
            "View all tests for Dataset openai-rag-f05a6fe8 at:\n",
            "https://smith.langchain.com/o/69867b2b-1696-431d-a878-81df6e9d559b/datasets/da76ed52-268a-4504-9f84-a5aa4030c45e\n",
            "[------------------------------------------------->] 9/9\n",
            " Eval quantiles:\n",
            "                                                   output  \\\n",
            "count                                                   9   \n",
            "unique                                                  9   \n",
            "top     When training and fine-tuning your own Large L...   \n",
            "freq                                                    1   \n",
            "mean                                                  NaN   \n",
            "std                                                   NaN   \n",
            "min                                                   NaN   \n",
            "25%                                                   NaN   \n",
            "50%                                                   NaN   \n",
            "75%                                                   NaN   \n",
            "max                                                   NaN   \n",
            "\n",
            "        feedback.correctness  feedback.embedding_cosine_distance  \\\n",
            "count                    9.0                        9.000000e+00   \n",
            "unique                   NaN                                 NaN   \n",
            "top                      NaN                                 NaN   \n",
            "freq                     NaN                                 NaN   \n",
            "mean                     1.0                        6.839050e-03   \n",
            "std                      0.0                        8.187693e-03   \n",
            "min                      1.0                       -2.220446e-16   \n",
            "25%                      1.0                        3.422211e-04   \n",
            "50%                      1.0                        4.193719e-03   \n",
            "75%                      1.0                        1.274014e-02   \n",
            "max                      1.0                        2.285098e-02   \n",
            "\n",
            "        feedback.relevance  feedback.fully_answered  feedback.dopeness_score  \\\n",
            "count             9.000000                 9.000000                 9.000000   \n",
            "unique                 NaN                      NaN                      NaN   \n",
            "top                    NaN                      NaN                      NaN   \n",
            "freq                   NaN                      NaN                      NaN   \n",
            "mean              0.888889                 0.777778                 0.872222   \n",
            "std               0.333333                 0.440959                 0.079495   \n",
            "min               0.000000                 0.000000                 0.700000   \n",
            "25%               1.000000                 1.000000                 0.850000   \n",
            "50%               1.000000                 1.000000                 0.900000   \n",
            "75%               1.000000                 1.000000                 0.900000   \n",
            "max               1.000000                 1.000000                 1.000000   \n",
            "\n",
            "       error  execution_time  \n",
            "count      0        9.000000  \n",
            "unique     0             NaN  \n",
            "top      NaN             NaN  \n",
            "freq     NaN             NaN  \n",
            "mean     NaN       35.007123  \n",
            "std      NaN       16.103949  \n",
            "min      NaN       21.568680  \n",
            "25%      NaN       24.292425  \n",
            "50%      NaN       28.163892  \n",
            "75%      NaN       37.462027  \n",
            "max      NaN       69.351234  \n"
          ]
        }
      ],
      "source": [
        "from langchain.smith import (\n",
        "    arun_on_dataset,\n",
        ")\n",
        "\n",
        "tag_name = f\"Rerank-EnsembleRetrieval\"\n",
        "tag = \"OpenAI Blog RAG -\" + tag_name\n",
        "\n",
        "chain_results = await arun_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=chain_factory,\n",
        "    evaluation=evaluation_config,\n",
        "    verbose=True,\n",
        "    tags=[tag],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simple_retriever = qdrant.as_retriever(search_kwargs={\"k\" : 5})"
      ],
      "metadata": {
        "id": "Wwh0uIIZF3vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naive_chain_factory():\n",
        "    rerank_rag_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | simple_retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    )\n",
        "    return rerank_rag_chain"
      ],
      "metadata": {
        "id": "b4lcfdX4FqQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tag_name = f\"SimpleRetriever\"\n",
        "tag = \"OpenAI Blog RAG -\" + tag_name\n",
        "\n",
        "chain_results = await arun_on_dataset(\n",
        "    client=client,\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=naive_chain_factory,\n",
        "    evaluation=evaluation_config,\n",
        "    verbose=True,\n",
        "    tags=[tag],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyNLmMgDF-jg",
        "outputId": "fb288e7d-88b7-4c89-e9c7-2ad300239804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'best-time-82' at:\n",
            "https://smith.langchain.com/o/69867b2b-1696-431d-a878-81df6e9d559b/datasets/da76ed52-268a-4504-9f84-a5aa4030c45e/compare?selectedSessions=1eef14c6-7ce9-4b36-a8b3-ea6741e39233\n",
            "\n",
            "View all tests for Dataset openai-rag-f05a6fe8 at:\n",
            "https://smith.langchain.com/o/69867b2b-1696-431d-a878-81df6e9d559b/datasets/da76ed52-268a-4504-9f84-a5aa4030c45e\n",
            "[------------------------------------------------->] 9/9\n",
            " Eval quantiles:\n",
            "                                                   output  \\\n",
            "count                                                   9   \n",
            "unique                                                  9   \n",
            "top     When training and fine-tuning your own Large L...   \n",
            "freq                                                    1   \n",
            "mean                                                  NaN   \n",
            "std                                                   NaN   \n",
            "min                                                   NaN   \n",
            "25%                                                   NaN   \n",
            "50%                                                   NaN   \n",
            "75%                                                   NaN   \n",
            "max                                                   NaN   \n",
            "\n",
            "        feedback.correctness  feedback.embedding_cosine_distance  \\\n",
            "count               9.000000                        9.000000e+00   \n",
            "unique                   NaN                                 NaN   \n",
            "top                      NaN                                 NaN   \n",
            "freq                     NaN                                 NaN   \n",
            "mean                0.888889                        2.710561e-02   \n",
            "std                 0.333333                        2.859967e-02   \n",
            "min                 0.000000                       -2.220446e-16   \n",
            "25%                 1.000000                        1.034801e-03   \n",
            "50%                 1.000000                        1.606963e-02   \n",
            "75%                 1.000000                        5.375840e-02   \n",
            "max                 1.000000                        7.254670e-02   \n",
            "\n",
            "        feedback.relevance  feedback.fully_answered  feedback.dopeness_score  \\\n",
            "count             9.000000                 9.000000                 9.000000   \n",
            "unique                 NaN                      NaN                      NaN   \n",
            "top                    NaN                      NaN                      NaN   \n",
            "freq                   NaN                      NaN                      NaN   \n",
            "mean              0.333333                 0.888889                 0.877778   \n",
            "std               0.500000                 0.333333                 0.071200   \n",
            "min               0.000000                 0.000000                 0.700000   \n",
            "25%               0.000000                 1.000000                 0.900000   \n",
            "50%               0.000000                 1.000000                 0.900000   \n",
            "75%               1.000000                 1.000000                 0.900000   \n",
            "max               1.000000                 1.000000                 0.950000   \n",
            "\n",
            "       error  execution_time  \n",
            "count      0        9.000000  \n",
            "unique     0             NaN  \n",
            "top      NaN             NaN  \n",
            "freq     NaN             NaN  \n",
            "mean     NaN       14.396443  \n",
            "std      NaN        8.995670  \n",
            "min      NaN        3.762634  \n",
            "25%      NaN        9.516934  \n",
            "50%      NaN       12.552749  \n",
            "75%      NaN       19.362962  \n",
            "max      NaN       31.734149  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "aims-visibility",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}